{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# State Value Function\n",
    "\n",
    "We want to calculate $V_{\\pi}(s)$ (the state-value-function given a policy)\n",
    "![mdp.png](mdp.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.1 Policy Evaluation by Dynamic Programming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "\n",
    "gamma = 0.1\n",
    "\n",
    "policy=np.array([[0.3, 0.2, 0.5], [0.5, 0.4, 0.1], [0.8, 0.1, 0.1]])\n",
    "# 'raw_rewards' variable contains rewards obtained after transition to each state\n",
    "# In our example it doesn't depend on source state\n",
    "raw_rewards = np.array([1.5, -1.833333333, 19.833333333])\n",
    "# 'rewards' variable contains expected values of the next reward for each state\n",
    "rewards = np.matmul(policy, raw_rewards)\n",
    "assert np.allclose(rewards, np.array([10., 2., 3.]))\n",
    "\n",
    "state_value_function=np.array([0 for i in range(3)])\n",
    "\n",
    "for i in range(20):\n",
    "    print(state_value_function)\n",
    "    state_value_function = rewards + gamma * (np.matmul(policy, state_value_function))\n",
    "\n",
    "    # state_value_function= np.matmul(rewards + 0.1*state_value_function,policy)\n",
    "print(state_value_function)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 0]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'gamma' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_144033/289007963.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_value_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mstate_value_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_value_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# state_value_function= np.matmul(rewards + 0.1*state_value_function,policy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gamma' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.1 Policy Evaluation by Linear Programming\n",
    "\n",
    "The state-value-function can be directly solved through linear programming (as shown on page 15):\n"
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "solution= np.matmul(np.linalg.inv(np.eye(3) - 0.1 * policy) , rewards)  #TODO: Implement the linear programming solution with a discount rate of 0.1\n",
    "print(solution)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[10.56602539  2.67438817  3.91113729]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result stays the same."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.2 Monte Carlo Policy Evaluation\n",
    "\n",
    "\n",
    "Monte Carlo Policy Evaluation can also be used, whereby sampling is used to get to the same result"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "reward_counter=np.array([0., 0., 0.])\n",
    "visit_counter=np.array([0., 0., 0.])\n",
    "\n",
    "def gt(rewardlist, gamma=0.1):\n",
    "    '''\n",
    "    Function to calculate the total discounted reward\n",
    "    >>> gt([10, 2, 3], gamma=0.1)\n",
    "    10.23\n",
    "    '''\n",
    "    #TODO: Implement the total discounted reward\n",
    "    return np.sum([gamma**i * val for i, val in enumerate(rewardlist)])\n",
    "\n",
    "for i in range(400):\n",
    "    start_state=random.randint(0, 2)\n",
    "    next_state=start_state\n",
    "    rewardlist=[]\n",
    "    occurence=defaultdict(list) \n",
    "    for i in range(250):\n",
    "        rewardlist.append(rewards[next_state]) \n",
    "        occurence[next_state].append(len(rewardlist)-1) \n",
    "        action=np.random.choice(np.arange(0, 3), p=policy[next_state]) \n",
    "        next_state=action\n",
    "\n",
    "    for state in occurence: \n",
    "        for value in occurence[state]: \n",
    "            rew=gt(rewardlist[value:]) \n",
    "            reward_counter[state]+=rew \n",
    "            visit_counter[state]+=1 \n",
    "            #break #if break: return following only the first visit\n",
    "\n",
    "print(reward_counter/visit_counter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10.219999999999999\n",
      "[10.56195692  2.67172578  3.90665637]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen the result is nearly the same as the state-value-function calculated above."
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.2 Policy Optimization by Q-Learning\n",
    "\n",
    "This code solves a very easy problem: using the rewards it calculated the optimal action-value-function.\n",
    "\n",
    "It samples a state-action pair randomly, so that all state-action pairs can be seen."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "q_table=np.zeros((3, 3)) \n",
    "lr = 0.8\n",
    "gamma = 0.1\n",
    "for i in range(1001): \n",
    "    state=random.randint(0, 2) \n",
    "    action=random.randint(0, 2) \n",
    "    next_state=action\n",
    "    reward=rewards[next_state] \n",
    "    next_q=max(q_table[next_state]) \n",
    "    q_table[state, action]= (1- lr) * q_table[state, action] + lr * (reward+gamma*next_q)\n",
    "    if i%100==0:\n",
    "        print(q_table)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.  0.  0. ]\n",
      " [0.  0.  0. ]\n",
      " [0.  0.  2.4]]\n",
      "[[11.1111019   3.1107329   4.11104615]\n",
      " [11.11063231  3.11102029  4.11102846]\n",
      " [11.11051902  3.11093618  4.11104819]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.1111111 ]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n",
      "[[11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]\n",
      " [11.11111111  3.11111111  4.11111111]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.3 Score Function Gradient Estimator\n",
    "Implement the score function gradient estimator in lxmls/reinforcement_learning/score\\_function\\_estimator.py. Check it is correct by calling the train() function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "from lxmls.reinforcement_learning.score_function_estimator import train\n",
    "train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Value Iteration\n",
    "\n"
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards=np.array([10., 2., 3.])\n",
    "gamma = 0.1\n",
    "\n",
    "state_value_function = np.zeros(3)\n",
    "\n",
    "for i in range(1000):\n",
    "    for s in range(3):\n",
    "        state_value_function[s]=#TODO: Implement the state value function update\n",
    "print(state_value_function)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.4 Policy Gradient for the CartPole task\n",
    "Implement policy gradient for the cartpole task by coding the forward pass of Model() in lxmls/reinforcement\\_learning/policy\\_gradient.py. Check it is correct by calling the train() function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from lxmls.reinforcement_learning.policy_gradient import train\n",
    "train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env.action_space Discrete(2)\n",
      "env.observation_space Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "env.observation_space.high [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "env.observation_space.low [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Complete exercise 6.4",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_144009/1465477967.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlxmls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforcement_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_gradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Repositories/lx_mls/lxmls-toolkit/lxmls/reinforcement_learning/policy_gradient.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m             action = int(np.random.choice(\n\u001b[1;32m     60\u001b[0m                 \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 p=np.exp(policy(observation).data.numpy()[0]))\n\u001b[0m\u001b[1;32m     62\u001b[0m             )\n\u001b[1;32m     63\u001b[0m             \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lxmls/lib/python3.9/site-packages/torch-1.9.0-py3.9-linux-x86_64.egg/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repositories/lx_mls/lxmls-toolkit/lxmls/reinforcement_learning/policy_gradient.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Solution to Exercise 6.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;31m# raise Exception(\"Complete exercise 6.4\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# End of solution to Exercise 6.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Complete exercise 6.4"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extra: Actor Critic for the CartPole task\n",
    "Implement actor crtitic for the cartpole task by coding the critic forward pass in lxmls/reinforcement\\_learning/policy\\_gradient.py. Check it is correct by calling the train() function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from lxmls.reinforcement_learning.actor_critic import train\n",
    "train()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}